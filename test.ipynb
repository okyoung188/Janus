{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom janus.utils.io import load_pil_images\n\n# specify the path to the model\nmodel_path = \"deepseek-ai/Janus-Pro-7B\"\nvl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\n\nvl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n    model_path, trust_remote_code=True\n)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n\nconversation = [\n    {\n        \"role\": \"<|User|>\",\n        \"content\": f\"<image_placeholder>\\n{question}\",\n        \"images\": [image],\n    },\n    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n]\n\n# load images and prepare for inputs\npil_images = load_pil_images(conversation)\nprepare_inputs = vl_chat_processor(\n    conversations=conversation, images=pil_images, force_batchify=True\n).to(vl_gpt.device)\n\n# # run image encoder to get the image embeddings\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n\n# # run the model to get the response\noutputs = vl_gpt.language_model.generate(\n    inputs_embeds=inputs_embeds,\n    attention_mask=prepare_inputs.attention_mask,\n    pad_token_id=tokenizer.eos_token_id,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    max_new_tokens=512,\n    do_sample=False,\n    use_cache=True,\n)\n\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T07:20:53.946332Z","iopub.execute_input":"2025-01-30T07:20:53.946589Z","iopub.status.idle":"2025-01-30T07:21:05.298488Z","shell.execute_reply.started":"2025-01-30T07:20:53.946564Z","shell.execute_reply":"2025-01-30T07:21:05.296780Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3e9766fc7b28>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjanus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiModalityCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVLChatProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjanus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_pil_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janus'"],"ename":"ModuleNotFoundError","evalue":"No module named 'janus'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"ls -al","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T07:30:48.792987Z","iopub.execute_input":"2025-01-30T07:30:48.793371Z","iopub.status.idle":"2025-01-30T07:30:48.928626Z","shell.execute_reply.started":"2025-01-30T07:30:48.793337Z","shell.execute_reply":"2025-01-30T07:30:48.926898Z"}},"outputs":[{"name":"stdout","text":"total 12\ndrwxr-xr-x 3 root root 4096 Jan 30 07:20 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 5 root root 4096 Jan 30 07:20 \u001b[01;34m..\u001b[0m/\ndrwxr-xr-x 2 root root 4096 Jan 30 07:20 \u001b[01;34m.virtual_documents\u001b[0m/\n","output_type":"stream"}],"execution_count":4}]}